{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrap_data(wiki_link, save_file_path = \"./input_data/movie.txt\"):\n",
    "    wikipedia_movie_link = wiki_link\n",
    "    \n",
    "    page_to_scrape = requests.get(wikipedia_movie_link)\n",
    "    soup = BeautifulSoup(page_to_scrape.text, \"html.parser\")\n",
    "    \n",
    "    para = ''\n",
    "    for paragraph in soup.select('p'):\n",
    "        p = paragraph.getText()\n",
    "        para += p\n",
    "\n",
    "\n",
    "     # Open the file in write mode and save the paragraph\n",
    "    with open(save_file_path, 'w') as file:\n",
    "        file.write(para)\n",
    "    \n",
    "    # print(f\"Paragraph saved to {save_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_aspect(aspect, content, save_file_path = \"./input_data/movie.txt\"):\n",
    "    # print(f\"from save aspect, {aspect}\")\n",
    "    introduction = f\"HERE IS THE DETAILS OF MOVIE'S {aspect.upper()}: \\n\"\n",
    "    underline = \"-----------------------------------------------------\\n\"\n",
    "    para = introduction + underline + content\n",
    "    # print(para)\n",
    "    # input()\n",
    "    # print(para)\n",
    "    # Open the file in write mode and save the paragraph\n",
    "    with open(save_file_path, 'w') as file:\n",
    "        file.write(para)\n",
    "    \n",
    "    # print(f\"Paragraph saved to {save_file_path} for {aspect}.\")\n",
    "\n",
    "\n",
    "def scrape(wiki_link):\n",
    "    Aspects = [\n",
    "        'Plot',\n",
    "        'Cast',\n",
    "        'Production',\n",
    "        'Music',\n",
    "        'Soundtrack',\n",
    "        'Themes',\n",
    "        'Accolades',\n",
    "        ]\n",
    "\n",
    "    Aspects = [aspect.lower() for aspect in Aspects]\n",
    "    \n",
    "    # Send a request to the Wikipedia page\n",
    "    response = requests.get(wiki_link)\n",
    "    \n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    scraped_dict = {}\n",
    "    \n",
    "    #-------------------Summary scraping--------------------------\n",
    "    # Find the first paragraph after the title, which is usually the summary\n",
    "    summary_paragraphs = []\n",
    "    # Wikipedia's summary paragraphs are inside <p> tags but before any <h2> tag\n",
    "    for paragraph in soup.find_all('p'):\n",
    "        \n",
    "        # Ensure the paragraph has text and is not empty\n",
    "        if paragraph.get_text().strip():\n",
    "            summary_paragraphs.append(paragraph.get_text().strip())\n",
    "        \n",
    "        # Stop once we hit the first section heading (e.g. 'Plot' or 'Contents')\n",
    "        if paragraph.find_next_sibling(['h2', 'h3']):\n",
    "            break\n",
    "    \n",
    "    scraped_dict[\"summary\"] =  ' '.join(summary_paragraphs)\n",
    "    \n",
    "    \n",
    "    # -------------------Other Aspect Scraping---------------------\n",
    "    Headings = soup.find_all('div', class_ = 'mw-heading mw-heading2')\n",
    "    for heading in Headings:\n",
    "        try:\n",
    "            aspect, _ = (heading.text).split('[')\n",
    "        except:\n",
    "            aspect = heading.text\n",
    "            \n",
    "        if aspect.lower() in Aspects:\n",
    "            next_siblings = heading.find_next_siblings()\n",
    "            text = ''\n",
    "            for next_sibling in next_siblings:\n",
    "                next_sibling_name = next_sibling.name\n",
    "                # sub_sibling = ''\n",
    "                # print(f\"............next_sibling_name = {next_sibling_name}\")\n",
    "                if (next_sibling_name =='style'):\n",
    "                    continue\n",
    "                \n",
    "                elif (next_sibling_name == 'div'):\n",
    "                    clss = next_sibling.get('class')\n",
    "                    \n",
    "                    if ('mw-heading2' in clss):  # break because heading ended\n",
    "                        break\n",
    "                text += \" \"+ next_sibling.text\n",
    "            scraped_dict[aspect] = text \n",
    "    return scraped_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader, UnstructuredPDFLoader, PyPDFium2Loader\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input data directory\n",
    "inputdirectory = Path(f\"./input_data\")\n",
    "## This is where the output csv files will be written\n",
    "outputdirectory = Path(f\"./output_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sudo apt-get install libmagic1\n",
    "# curl -fsSL https://ollama.com/install.sh | sh\n",
    "#ollama serve\n",
    "#ollama run zephyr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openpyxl\n",
    "# !pip install pandas\n",
    "# !pip install langchain\n",
    "# !pip install -U langchain-community\n",
    "# !pip install unstructured\n",
    "# !pip install yachalk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strat from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openpyxl\n",
    "import uuid\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import json\n",
    "import ollama.client as client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_df(documents) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for chunk in documents:\n",
    "        row = {\n",
    "            \"text\": chunk.page_content,\n",
    "            **chunk.metadata,\n",
    "            \"chunk_id\": uuid.uuid4().hex,\n",
    "        }\n",
    "        rows = rows + [row]\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(input: str, metadata={}, model=\"mistral-openorca:latest\"):\n",
    "    if model == None:\n",
    "        model = \"mistral-openorca:latest\"\n",
    "\n",
    "    # model_info = client.show(model_name=model)\n",
    "    # print( chalk.blue(model_info))\n",
    "\n",
    "\n",
    "\n",
    "    SYS_PROMPT = (\n",
    "        \"You create network graphs by identifying terms and their relationships within a given context. \"\n",
    "        \"You are given a context chunk (enclosed by ```). Your task is to identify the key concepts mentioned\" \n",
    "        \"in the context and extract their ontology. \\n\"\n",
    "        \n",
    "        \n",
    "        \"Thought 1: When analyzing the text, carefully identify significant terms from each sentence. \\n\" \n",
    "        \"These terms should represent essential elements such as objects, places, people, organizations, \\n\" \n",
    "        \"concepts, acronums, or services. Aim to extract terms in their simplest form, ensuring each one captures a core idea without unncessary compleity. \\n\\n\"\n",
    "        \n",
    "      \n",
    "       \"Thought 2: Once the significant terms are identified, evaluate how these terms connect. \\n\" \n",
    "       \"Consider terms that appear together in the same sentence or paragraph, as their proximity often indicates \\n\" \n",
    "       \"a logical relationship. Keep in mind that a single term may relate to multiple others, reflecting the interconnected nature of the text. \\n\\n\"\n",
    "       \n",
    "\n",
    "       \"Thought 3: For every pair of connected terms, identify and clearly specify the type of relationship that exists between them.\\n\" \n",
    "       \"Use short, precise descriptions or labels to define these relationships. Represent the connections in a structured format, \\n\" \n",
    "       \"such as a list of JSON objects, wehre each entry consists of two related terms (nodes) and the description of their relationship (edge). \\n\\n\"\n",
    "\n",
    "\n",
    "        \"[\\n\"\n",
    "        \"   {\\n\"        \n",
    "        '       \"node_1\": \"A term identified from the ontology\",\\n'\n",
    "        '       \"node_2\": \"A related term identified from the ontology\",\\n'\n",
    "        '       \"edge\": \"the connection or relationship between node_1 and node_2 described in one or two sentences\"\\n'\n",
    "        \"   }, {...}\\n\"\n",
    "        \"]\"\n",
    "        \n",
    "        \"Don't allow any effect of your previous response\\n\"\n",
    "        \"Take your time as much as you require in generating the response\\n\"\n",
    "        \"Ensure the edge information will contain at leat 5 words including the node_1 and Node_2.\\n\"\n",
    "\n",
    "    )\n",
    "\n",
    "    USER_PROMPT = f\"context: ```{input}``` \\n\\n output: \"\n",
    "    response, _ = client.generate(model_name=model, system=SYS_PROMPT, prompt=USER_PROMPT)\n",
    "    try:\n",
    "        result = json.loads(response)\n",
    "        result = [dict(item, **metadata) for item in result]\n",
    "    except:\n",
    "        # print(\"\\n\\nERROR ### Here is the buggy response: \", response, \"\\n\\n\")\n",
    "        print(f\"RESULT TYPE = {type(result)}\")\n",
    "        result = None\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_grph(dataframe: pd.DataFrame, model=None) -> list:\n",
    "    # dataframe.reset_index(inplace=True)\n",
    "    results = dataframe.apply(\n",
    "        lambda row: prompt(row.text, {\"chunk_id\": row.chunk_id}, model), axis=1\n",
    "    )\n",
    "    # invalid json results in NaN\n",
    "    results = results.dropna()\n",
    "    results = results.reset_index(drop=True)\n",
    "\n",
    "    ## Flatten the list of lists to one single list of entities.\n",
    "    concept_list = np.concatenate(results).ravel().tolist()\n",
    "    return concept_list\n",
    "\n",
    "\n",
    "def grph_to_df(nodes_list) -> pd.DataFrame:\n",
    "    ## Remove all NaN entities\n",
    "    graph_dataframe = pd.DataFrame(nodes_list).replace(\" \", np.nan)\n",
    "    graph_dataframe = graph_dataframe.dropna(subset=[\"node_1\", \"node_2\"])\n",
    "    graph_dataframe[\"node_1\"] = graph_dataframe[\"node_1\"].apply(lambda x: x.lower())\n",
    "    graph_dataframe[\"node_2\"] = graph_dataframe[\"node_2\"].apply(lambda x: x.lower())\n",
    "\n",
    "    return graph_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation for bollywood movies. change the \"bollywood\" to \"hollywood\" when required\n",
    "columns = [\"YoR\", \"movie_name\", \"imdb_rating\", \"wiki_link\", \"popular\"]\n",
    "movie_links = pd.read_excel(\"./Movie_list.xlsx\", sheet_name = \"bollywood\", engine='openpyxl')  # all the file containing movie's wiki link\n",
    "movie_links.columns = columns\n",
    "# movie_links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_movie_links = movie_links[movie_links.popular == \"popular\"]\n",
    "least_popular_movie_links = movie_links[movie_links.popular == \"Least popular\"]\n",
    "# least_popular_movie_links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root_output_folder = \"./bollywood\"\n",
    "movie_categories = [least_popular_movie_links, popular_movie_links]\n",
    "\n",
    "for movie_category in tqdm(movie_categories):\n",
    "    for index, row in movie_category.iterrows():\n",
    "        \n",
    "        movie_name = row[\"movie_name\"]\n",
    "        YoR = row[\"YoR\"]\n",
    "        wiki_link = row[\"wiki_link\"]\n",
    "        popular = row[\"popular\"]\n",
    "\n",
    "        # scrape data from the given link aspectwise and get it in a dictionary:\n",
    "        aspect_dict = scrape(wiki_link)\n",
    "        \n",
    "        # Now, generate nodes from each aspect individually after saving their content in the data_input directory one by one.\n",
    "        for aspect in aspect_dict:\n",
    "            save_aspect(aspect, aspect_dict[aspect])\n",
    "\n",
    "            try:\n",
    "\n",
    "                #load document in dataframe chunk\n",
    "                loader = DirectoryLoader(inputdirectory, show_progress=True)\n",
    "                documents = loader.load()\n",
    "                \n",
    "                splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=1500,\n",
    "                    chunk_overlap=150,\n",
    "                    length_function=len,\n",
    "                    is_separator_regex=False,\n",
    "                )            \n",
    "                pages = splitter.split_documents(documents)\n",
    "    \n",
    "                \n",
    "                # Create dataframe of chunks\n",
    "                df = doc_to_df(pages)\n",
    "    \n",
    "                #node generation task\n",
    "                ## To regenerate the graph with LLM, set this to True\n",
    "                regenerate = True\n",
    "                \n",
    "                if regenerate:\n",
    "                    concepts_list = df_to_grph(df, model='zephyr:latest')\n",
    "                    dfg1 = grph_to_df(concepts_list)\n",
    "                    if not os.path.exists(outputdirectory):\n",
    "                        os.makedirs(outputdirectory)\n",
    "                    \n",
    "                    dfg1.to_csv(outputdirectory/\"graph.csv\", sep=\"|\", index=False)\n",
    "                    df.to_csv(outputdirectory/\"chunks.csv\", sep=\"|\", index=False)\n",
    "                else:\n",
    "                    dfg1 = pd.read_csv(outputdirectory/\"graph.csv\", sep=\"|\")\n",
    "                \n",
    "                dfg1.replace(\"\", np.nan, inplace=True)\n",
    "                dfg1.dropna(subset=[\"node_1\", \"node_2\", 'edge'], inplace=True)\n",
    "                dfg1['count'] = 4  \n",
    "                \n",
    "                #save the nodes dataframe in csv_file\n",
    "                save_folder_name = popular\n",
    "                save_file_name = movie_name + \"_\" + str(YoR) + \"_\" + aspect +\".csv\"\n",
    "                save_path = os.path.join(root_output_folder, save_folder_name, save_file_name) \n",
    "                # print(f\"SAVE PATH = {save_path}\")\n",
    "                # break\n",
    "                dfg1.to_csv(save_path, index=False)\n",
    "\n",
    "            except:\n",
    "                continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
